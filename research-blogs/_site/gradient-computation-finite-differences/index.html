<!DOCTYPE html>
<html lang="en">
<head>
  <!-- for mathjax support -->
  
      <script type="text/x-mathjax-config">
        MathJax.Hub.Config({
        TeX: { equationNumbers: { autoNumber: "AMS" } }
        });
      </script>
      <script type="text/javascript" async src="http://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script>
    
	<meta charset="utf-8">
	<title>Finite differences with simultaneous perturbation - Research blogs</title>

  <!-- Edit site and author settings in `_config.yml` to make the social details your own -->

    <meta content="Research blogs" property="og:site_name">
  
    <meta content="Finite differences with simultaneous perturbation" property="og:title">
  
  
    <meta content="article" property="og:type">
  
  
    <meta content="You’ll find this post in your `_posts` directory. Go ahead and edit it and re-build the site to see your changes." property="og:description">
  
  
    <meta content="http://localhost:4000/gradient-computation-finite-differences/" property="og:url">
  
  
    <meta content="2022-04-28T17:15:20-04:00" property="article:published_time">
    <meta content="http://localhost:4000/about/" property="article:author">
  
  
    <meta content="http://localhost:4000/research-blogs/assets/img/gradient.png" property="og:image">
  
  
    
  
  
    
    <meta content="Optimization" property="article:tag">
    
    <meta content="Gradient computation" property="article:tag">
    
  

    <meta name="twitter:card" content="summary">
    <meta name="twitter:site" content="@">
    <meta name="twitter:creator" content="@">
  
    <meta name="twitter:title" content="Finite differences with simultaneous perturbation">
  
  
    <meta name="twitter:url" content="http://localhost:4000/gradient-computation-finite-differences/">
  
  
    <meta name="twitter:description" content="You’ll find this post in your `_posts` directory. Go ahead and edit it and re-build the site to see your changes.">
  
  
    <meta name="twitter:image:src" content="http://localhost:4000/research-blogs/assets/img/gradient.png">
  

	<meta name="description" content="You’ll find this post in your `_posts` directory. Go ahead and edit it and re-build the site to see your changes.">
	<meta http-equiv="X-UA-Compatible" content="IE=edge">
	<meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1">
	<meta property="og:image" content="">
	<link rel="shortcut icon" href="/research-blogs/assets/img/favicon/favicon.ico" type="image/x-icon">
	<link rel="apple-touch-icon" href="/research-blogs/assets/img/favicon/apple-touch-icon.png">
	<link rel="apple-touch-icon" sizes="72x72" href="/research-blogs/assets/img/favicon/apple-touch-icon-72x72.png">
	<link rel="apple-touch-icon" sizes="144x144" href="/research-blogs/assets/img/favicon/apple-touch-icon-144x144.png">
	<!-- Chrome, Firefox OS and Opera -->
	<meta name="theme-color" content="#263959">
	<!-- Windows Phone -->
	<meta name="msapplication-navbutton-color" content="#263959">
	<!-- iOS Safari -->
	<meta name="apple-mobile-web-app-status-bar-style" content="#263959">
	<!-- Google Fonts -->
	<link href="https://fonts.googleapis.com/css?family=PT+Serif:400,700|Lato:300,400,700&display=swap" rel="stylesheet">
	<!-- Font Awesome -->
	<link rel="stylesheet" href="/research-blogs/assets/fonts/font-awesome/css/font-awesome.min.css">
	<!-- Styles -->
	<link rel="stylesheet" href="/research-blogs/assets/css/main.css">
</head>

<body>

  <div class="wrapper">
    <aside class="sidebar">
  <header>
    <div class="about">
      <div class="cover-author-image">
        <a href="/research-blogs/"><img src="/research-blogs/assets/img/raihan_profile.jpg" alt="Raihan Seraj"></a>
      </div>
      <div class="author-name">Raihan Seraj</div>
      <p>I write about my experiences and things that I have learned during my journey as a research student.</p>
    </div>
  </header> <!-- End Header -->
  <footer>
    <section class="contact">
      <h3 class="contact-title">Contact me</h3>
      <ul>
        
          <li><a href="https://twitter.com/artemsheludko_" target="_blank"><i class="fa fa-twitter" aria-hidden="true"></i></a></li>
        
        
          <li><a href="https://facebook.com/raihan.seraj# add your Facebook handle" target="_blank"><i class="fa fa-facebook" aria-hidden="true"></i></a></li>
        
        
          <li class="github"><a href="http://github.com/Raihan-Seraj" target="_blank"><i class="fa fa-github"></i></a></li>
        
        
          <li class="linkedin"><a href="https://in.linkedin.com/in/raihan-seraj# add your Linkedin handle" target="_blank"><i class="fa fa-linkedin"></i></a></li>
        
        
          <li class="email"><a href="mailto:raihan.seraj@mail.mcgill.ca# add your Email address"><i class="fa fa-envelope-o"></i></a></li>
        
      </ul>
    </section> <!-- End Section Contact -->
    <div class="copyright">
      <p>2022 &copy; Raihan Seraj</p>
    </div>
  </footer> <!-- End Footer -->
</aside> <!-- End Sidebar -->
<div class="content-box clearfix">
  <article class="article-page">
  <div class="page-content">
    
    <div class="page-cover-image">
      <figure>
        <img class="page-image" src=/research-blogs/assets/img/gradient.png alt="Finite differences with simultaneous perturbation">
        
      </figure>
    </div> <!-- End Page Cover Image -->
    
    <div class="wrap-content">
      <header class="header-page">
        <h1 class="page-title">Finite differences with simultaneous perturbation</h1>
        <div class="page-date"><span>2022, Apr 28&nbsp;&nbsp;&nbsp;&nbsp;</span></div>
      </header>
      <h2 id="need-for-computing-gradients">Need for computing gradients</h2>
<p>Gradient descent is the most common optimization procedure that is used to optimize many machine learning algorithms. The core idea behind the gradient descent algorithm is to compute the gradient of the loss function or an objective function with respect to its parameters and iteratively update these parameters. Throughout my journey as a research student, I have often encountered objective functions with high dimensional parameter space where computing an analytic expression for the derivative becomes difficult. Ofcourse with the advent of deep neural networks as an effective non-linear function approximator and the availability of some excellent frameworks to implement them we naturally do not have to worry about computing the gradient of the loss function. This is automatically done using automatic differentiation packages which computes the gradient using chain rule.</p>

<h2 id="computing-gradient-with-finite-differences">Computing gradient with finite differences</h2>
<p>Finite differences is one of the oldest method use to solve differential equations. These methods are well understood where the 
central idea is to evaluate the objective function for two different parameter values and dividing them by their difference.</p>

<p>Consider the objective function defined as \(h(\theta)\), where \(\theta\) is the parameter of the function. Let \(\alpha = \theta+\epsilon\). Therefore the derivative of \(h\) at point \(\theta\)  is given by the limit</p>

<p>[\begin{aligned}h’(\theta)= \lim_{\epsilon\to 0}\dfrac{h(\alpha)-h(\theta)}{\epsilon}\end{aligned}]</p>

<p>I had some experiences of using finite difference methods for performance gradient estimation with Partially Observable Markov Decision Processes (POMDPs). Since I was using a stochastic finite state controller for POMDPs it was not straight forward for me to compute the performance gradient analytically. Therefore I used two finite difference algorithms.</p>
<ul>
  <li>Simultaneous Perturbation Stochastic Approximation (SPSA) [1].</li>
  <li>Smooth Function Approximation (SFA)[2].</li>
</ul>

<p>These two algorithms falls in the family of simultaneous perturbation algorithm where the gradient estimate \(\widehat\nabla h\) takes the form</p>

<p>[\begin{aligned}\widehat\nabla h = \dfrac{\delta \Bigl(h(\omega)-h(\theta)\Bigr)}{k}\end{aligned}]</p>

<p>where \(\omega = \theta + k\delta\) and \(k\) is a small constant. \(\delta\) is a random variable with the same size of \(\theta\). The subtle difference between these two algorithms is the way \(\delta\) is generated. For SFA, \(\delta\) is sampled from a normal distribution,i.e., \(\delta\sim \mathcal N(0,I)\) with  \(I\) being identity matrix. For SPSA \(\delta\) is sampled from a  Rademacher distribution \(\delta^{(i)}\sim Rademacher(+1,-1)\) (where \(\delta^{(i)}\) has a probability of \(0.5\) of being \(+1\) and \(-1\))</p>

<h2 id="practical-experience-and-limitations">Practical experience and limitations</h2>
<p>My practical experience in using both these algorithms is quite similar. Both these algorithms are fairly easy and straightforward to implement. One of the limitations of using both these algorithms or any finite differences algorithm is that 
you need to evaluate the objective function twice with difference parameters. This can get expensive if the function evaluation is itself expensive. The main takeaway for these family of algorithms is that they only provide an approximate estimate of the gradient. Thus, they might not work well if an algorithm requires an exact gradient computation. Nevertheless they provide a neat way of gradient estimation and are widely used for large-scale population models, simulation optimization and atmospheric modelling.</p>

<h2 id="references">References</h2>

<ol>
  <li>Spall, J. C. (1992). Multivariate stochastic approximation using a simultaneous perturbation gradient approximation. IEEE transactions on automatic control, 37(3), 332-341.</li>
  <li>Katkovnik, V. Y., &amp; KULCHITS. OY. (1972). Convergence of a class of random search algorithms. Automation and Remote Control, 33(8), 1321-1326.
<!-- Suppose we are optimizing a supervised training algorithm where we have access to the true label of the dataset. Let $$y^{(i)}$$ be the true label for the $$i^{th}$$ training example. For simplicity let us assume that the data set have a two dimensional feature space represented by $$x_1, x_2$$ where $$(x_1, x_2)\in \mathbb{R}$$. The learning algorithm is associated with a hypothesis where we assume the nature of the underlying function that represents the true value. For instance if we consider the hypothesis to be linear then we assume that the estimate of the true label $$y_{pred}$$ is some linear function of the features $$x_1,x_2$$. given by  $$\begin{equation}y_{pred}^{(i)}=\theta_1^(i)x_1+\theta_2x_2+\theta_3\end{equation}$$  --></li>
</ol>

<!-- ![I and My friends](/research-blogs/assets/img/we-in-rest.jpg) -->


      <div class="page-footer">
        <div class="page-share">
          <a href="https://twitter.com/intent/tweet?text=Finite differences with simultaneous perturbation&url=http://localhost:4000/gradient-computation-finite-differences/" title="Share on Twitter" rel="nofollow" target="_blank">Twitter</a>
          <a href="https://facebook.com/sharer.php?u=http://localhost:4000/gradient-computation-finite-differences/" title="Share on Facebook" rel="nofollow" target="_blank">Facebook</a>
          <a href="https://plus.google.com/share?url=http://localhost:4000/gradient-computation-finite-differences/" title="Share on Google+" rel="nofollow" target="_blank">Google+</a>
        </div>
        <div class="page-tag">
          
            <a href="/research-blogs/tags#Optimization" class="tag">&#35; Optimization</a>
          
            <a href="/research-blogs/tags#Gradient computation" class="tag">&#35; Gradient computation</a>
          
        </div>
      </div>
      <section class="comment-area">
  <div class="comment-wrapper">
    
  </div>
</section> <!-- End Comment Area -->

    </div> <!-- End Wrap Content -->
  </div> <!-- End Page Content -->
</article> <!-- End Article Page -->

</div>

  </div>
  
  <script>
  (function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
  (i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
  m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
  })(window,document,'script','https://www.google-analytics.com/analytics.js','ga');
  ga('create', '', 'auto');
  ga('send', 'pageview');
</script> <!-- End Analytics -->

</body>
</html>
